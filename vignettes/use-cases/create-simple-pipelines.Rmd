---
title: "Create Simple Pipelines"
output:
  html_document:
    df_print: paged
---

Once you have shared datasets (manually created by [Reusing Tidy Datasets](reuse-tidy-datasets.htm) or by automating them with [Scheduled Dataset Updates](schedule-dataset-updates.html)), you can also consider creating code that depends on one or many pins to further process datasets or pin other objects like visualizations, models, and so on.

For instance, we could use the `worldnews` pin to create a deep learning model on a daily schedule. One of the state-of-the-art language models is [GPT-2](https://openai.com/blog/better-language-models/), which we can also use from R through an experimental [gpt2](https://github.com/javierluraschi/gpt2/) package. First install the package and dependencies,

```{r eval=FALSE}
remotes::install_github("javierluraschi/gpt2")
gpt2::install_gpt2()
```

Then you can retrieve the previous automated pin, apply the GPT-2 text generation model and pin the result in a new pin -- which essentially creates a simple data processing pipeline:

```{r}
library(pins)

pin_get("worldnews", board = "rsconnect") %>%
  dplyr::mutate(generated = gpt2::gpt2(title)) %>%
  pin("news-generated", board = "rsconnect")
```
